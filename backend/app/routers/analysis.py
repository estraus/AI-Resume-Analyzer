from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse
from app.models import AnalysisResult, AgentUpdate
from app.services.analysis_service import AnalysisService
from app.pdf_parser import extract_text_from_pdf
from app.job_scraper import scrape_job_posting
import asyncio
import json
from typing import AsyncGenerator

router = APIRouter(prefix="/api", tags=["analysis"])

# Store for SSE connections
active_analyses = {}

@router.post("/analyze", response_model=dict)
async def analyze_resume(
    resume: UploadFile = File(...),
    job_url: str = Form(None),
    job_description: str = Form(None)
):
    """
    Analyze resume against job posting.

    Args:
        resume: PDF resume file
        job_url: Job posting URL (optional if job_description provided)
        job_description: Direct job description text (optional if job_url provided)

    Returns:
        Analysis ID for tracking progress
    """
    try:
        # Validate that either job_url or job_description is provided
        if not job_url and not job_description:
            raise HTTPException(status_code=400, detail="Either job_url or job_description is required")

        # Validate file type
        if not resume.filename.endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are supported")

        # Read PDF
        pdf_bytes = await resume.read()
        resume_text = extract_text_from_pdf(pdf_bytes)

        if not resume_text or len(resume_text) < 50:
            raise HTTPException(status_code=400, detail="Could not extract text from PDF")

        # Get job description - either from URL or directly provided
        if job_description:
            job_desc = job_description
        else:
            try:
                job_desc = scrape_job_posting(job_url)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"Could not scrape job URL: {str(e)}")

        # Create analysis service
        updates_queue = asyncio.Queue()

        async def progress_callback(agent: str, status: str, message: str, progress: int):
            await updates_queue.put(AgentUpdate(
                agent_name=agent,
                status=status,
                message=message,
                progress=progress
            ))

        service = AnalysisService(progress_callback=progress_callback)

        # Start analysis in background
        analysis_task = asyncio.create_task(
            service.analyze_resume(resume_text, job_desc)
        )

        # Get analysis ID from service
        analysis_id = "temp_id"  # Will be generated by service
        active_analyses[analysis_id] = {
            "task": analysis_task,
            "updates": updates_queue
        }

        return {"analysis_id": analysis_id, "status": "started"}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analysis/{analysis_id}/stream")
async def stream_analysis_updates(analysis_id: str):
    """
    Stream real-time updates for an analysis.

    Args:
        analysis_id: Analysis identifier

    Returns:
        Server-Sent Events stream
    """
    if analysis_id not in active_analyses:
        raise HTTPException(status_code=404, detail="Analysis not found")

    async def event_generator() -> AsyncGenerator:
        updates_queue = active_analyses[analysis_id]["updates"]
        task = active_analyses[analysis_id]["task"]

        while not task.done():
            try:
                # Wait for update with timeout
                update = await asyncio.wait_for(updates_queue.get(), timeout=1.0)
                yield {
                    "event": "update",
                    "data": update.model_dump_json()
                }
            except asyncio.TimeoutError:
                # Send keepalive
                yield {"event": "ping", "data": ""}

        # Send final result
        try:
            result = await task
            yield {
                "event": "complete",
                "data": result.model_dump_json()
            }
        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({"error": str(e)})
            }
        finally:
            # Cleanup
            del active_analyses[analysis_id]

    return EventSourceResponse(event_generator())


@router.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "service": "AI Resume Analyzer"}
